Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Peters2010,
abstract = {Policy gradient methods are a type of reinforcement learning techniques that rely upon optimizing parametrized policies with respect to the expected return (long-term cumulative reward) by gradient descent. They do not suffer from many of the problems that have been marring traditional reinforcement learning approaches such as the lack of guarantees of a value function, the intractability problem resulting from uncertain state information and the complexity arising from continuous states {\&} actions.},
author = {Peters, Jan},
doi = {10.4249/scholarpedia.3698},
isbn = {142440259X},
issn = {1941-6016},
journal = {Scholarpedia},
month = {nov},
number = {11},
pages = {3698},
title = {{Policy gradient methods}},
url = {http://www.scholarpedia.org/article/Policy{\_}gradient{\_}methods},
volume = {5},
year = {2010}
}
@article{Wold2001,
author = {Wold, Svante and Sjostrom, Michael},
file = {:E$\backslash$:/Nubes/Onedrive/OneDrive for Business/Proyectos de Investigaci{\'{o}}n/Proyecto-EARTH/Investigaci{\'{o}}n Bibliogr{\'{a}}fica/Wold - PLSRegression a basic tool of chemometrics.pdf:pdf},
keywords = {latent variables,multivariate analysis,pls,plsr,two-block predictive pls},
pages = {109--130},
title = {{PLS-regression : a basic tool of chemometrics}},
year = {2001}
}
@book{SistemaNacionaldeAcreditaciondelaEducacionSuperior2012,
address = {San Jos{\'{e}}, Costa Rica},
author = {{Sistema Nacional de Acreditaci{\'{o}}n de la Educaci{\'{o}}n Superior}},
file = {:C$\backslash$:/Users/kenno/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jos{\'{e}}, Rica - 2012 - GU{\'{I}}A PARA ELABORAR Y REVISAR EL INFORME DE AVANCE DE CUMPLIMIENTO DEL COMPROMISO DE MEJORAMIENTO.pdf:pdf},
title = {{GU{\'{I}}A PARA ELABORAR Y REVISAR EL INFORME DE AVANCE DE CUMPLIMIENTO DEL COMPROMISO DE MEJORAMIENTO}},
url = {https://www.sinaes.ac.cr/documentos/Guia{\_}elaborar{\_}y{\_}revisar{\_}ACCM.pdf},
year = {2012}
}
@article{Turing1950,
author = {Turing, Alan},
journal = {Mind},
pages = {433--460},
title = {{Computing machinery and intelligence}},
volume = {59},
year = {1950}
}
@article{Bengio2007,
abstract = {One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), rea- soning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, withmin- imal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally lim- ited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very ineffi- cient in terms of required number of computational elements and examples. Sec- ond, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learn- ing) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more ab- stract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bengio, Yoshua and {\{}LeCun{\}}, Yann and Lecun, Yann},
doi = {10.1.1.72.4580},
eprint = {arXiv:1011.1669v3},
isbn = {1002620262},
issn = {00099104},
journal = {Large Scale Kernel Machines},
number = {1},
pages = {321--360},
pmid = {11359439},
title = {{Scaling Learning Algorithms towards AI}},
year = {2007}
}
@article{Siegmann2015,
author = {Siegmann, Bastian and Jarmer, Thomas},
doi = {10.1080/01431161.2015.1084438},
file = {:E$\backslash$:/Nubes/Onedrive/OneDrive for Business/Proyectos de Investigaci{\'{o}}n/Proyecto-EARTH/Investigaci{\'{o}}n Bibliogr{\'{a}}fica/Comparison of different regression models and validation techniques for the assessment of wheat leaf area index fr.pdf:pdf},
issn = {0143-1161},
journal = {International Journal of Remote Sensing},
number = {18},
pages = {4519--4534},
publisher = {Taylor {\&} Francis},
title = {{Comparison of different regression models and validation techniques for the assessment of wheat leaf area index from hyperspectral data}},
url = {http://dx.doi.org/10.1080/01431161.2015.1084438},
volume = {36},
year = {2015}
}
@article{Garriga2017,
abstract = {Phenotyping, via remote and proximal sensing techniques, of the agronomic and physiological traits associated with yield potential and drought adaptation could contribute to improvements in breeding programs. In the present study, 384 genotypes of wheat (Triticum aestivum L.) were tested under fully irrigated (FI) and water stress (WS) conditions. The following traits were evaluated and assessed via spectral reflectance: grain yield (GY), spikes per square meter (SM2), kernels per spike (KPS), thousand-kernel weight (TKW), chlorophyll content (SPAD), stem water soluble carbohydrate concentration and content (WSC and WSCC, respectively), carbon isotope discrimination (∆13C), and leaf area index (LAI). The performances of spectral reflectance indices (SRIs), four regression algorithms (PCR, PLSR, ridge regression RR, and SVR), and three classification methods (PCA-LDA, PLS-DA, and kNN) were evaluated for the prediction of each trait. For the classification approaches, two classes were established for each trait: the lower 80{\%} of the trait variability range (Class 1) and the remaining 20{\%} (Class 2 or elite genotypes). Both the SRIs and regression methods performed better when data from FI and WS were combined. The traits that were best estimated by SRIs and regression methods were GY and ∆13C. For most traits and conditions, the estimations provided by RR and SVR were the same, or better than, those provided by the SRIs. PLS-DA showed the best performance among the categorical methods and, unlike the SRI and regression models, most traits were relatively well classified within a specific hydric condition (FI or WS), proving that classification approach is an effective tool to be explored in future studies related to genotype selection.},
author = {Garriga, Miguel and Romero-Bravo, Sebasti{\'{a}}n and Estrada, F{\'{e}}lix and Escobar, Alejandro and Matus, Iv{\'{a}}n A. and del Pozo, Alejandro and Astudillo, Cesar A. and Lobos, Gustavo A.},
doi = {10.3389/fpls.2017.00280},
file = {:C$\backslash$:/Users/kenno/Desktop/fpls-08-00280.pdf:pdf},
isbn = {1664-462X},
issn = {1664-462X},
journal = {Frontiers in Plant Science},
keywords = {13C,C13,PLS-DA,Phenomic,carbon isotope discrimination,high-throughput phenotyping,phenotyping,reflectance},
month = {mar},
pages = {280},
pmid = {28337210},
publisher = {Frontiers},
title = {{Assessing Wheat Traits by Spectral Reflectance: Do We Really Need to Focus on Predicted Trait-Values or Directly Identify the Elite Genotypes Group?}},
url = {http://journal.frontiersin.org/article/10.3389/fpls.2017.00280/full},
volume = {8},
year = {2017}
}
@book{James2013,
abstract = {Includes index. An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra. Introduction -- Statistical Learning -- Linear Regression -- Classification -- Resampling Methods -- Linear Model Selection and Regularization -- Moving Beyond Linearity -- Tree-Based Methods -- Support Vector Machines -- Unsupervised Learning.},
author = {{James, G., Wittrn, D., Hastie, T., {\&} Tibshirani}, R.},
doi = {10.1007/978-1-4614-7138-7},
isbn = {9781461471387},
title = {{An introduction to Statistical Learning with applications in R}},
url = {https://books.google.co.cr/books?id=qcI{\_}AAAAQBAJ{\&}printsec=frontcover{\#}v=onepage{\&}q{\&}f=false},
year = {2013}
}
@misc{perceptrons,
title = {{Perceptrons - the most basic form of a neural network {\textperiodcentered} Applied Go}},
url = {https://appliedgo.net/perceptron/},
urldate = {2018-04-08}
}
@misc{Whyte2014,
author = {Whyte, Aaron},
booktitle = {Google.Github.Io},
keywords = {as it,internet explorer,s not supported in},
pages = {1--24},
title = {{Google JavaScript Style Guide}},
url = {https://google.github.io/styleguide/jsguide.html},
urldate = {2018-03-08},
year = {2014}
}
@book{Rusell2010,
author = {Russell, S J and Norvig, P},
file = {:C$\backslash$:/Users/kenno/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sugawara, Nikaido - 2014 - Properties of AdeABC and AdeIJK efflux systems of Acinetobacter baumannii compared with those of the AcrAB-To.pdf:pdf},
isbn = {9780136042594},
publisher = {Prentice Hall},
series = {Prentice Hall series in artificial intelligence},
title = {{Artificial Intelligence: A Modern Approach}},
url = {https://books.google.co.cr/books?id=8jZBksh-bUMC},
year = {2010}
}
@article{Williams1992,
author = {Williams, Ronald J},
file = {:E$\backslash$:/IA/Reinforcement-Learning-FlappyBird/Papers/Williams-1992.pdf:pdf},
journal = {Machine Learning},
keywords = {connectionist networks,gradient descent,mathematical analysis,reinforcement learning},
pages = {229--256},
pmid = {903},
title = {{Simple statistical gradient following algorithms for connectionist reinforcement learning}},
volume = {8},
year = {1992}
}
@inproceedings{Papavassiliou1999,
abstract = {A key open problem in reinforcement learning is to assure convergence when using a compact hypothesis class to approximate the value function. Although the standard temporal-difference learning algorithm has been shown to converge when the hypothesis class is a linear combination of fixed basis functions, it may diverge with a general (nonlinear) hypothesis class. This paper describes the Bridge algorithm, a new method for reinforcement learning, and shows that it converges to an approximate global optimum for any agnostically learnable hypothesis class. Convergence is demonstrated on a simple example for which temporal-difference learning fails. Weak conditions are identified under which the Bridge algorithm converges for any hypothesis class. Finally, connections are made between the complexity of reinforcement learning and the PAC-learnability of the hypothesis class.},
author = {Papavassiliou, Vassilis A. and Russell, Stuart},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/kenno/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/-Papavassiliou, Russell - Unknown - Convergence of reinforcement learning with general function approximators.pdf:pdf},
issn = {10450823},
pages = {748--755},
title = {{Convergence of reinforcement learning with general function approximators}},
url = {https://www.ijcai.org/Proceedings/99-2/Papers/014 .pdf},
volume = {2},
year = {1999}
}
@book{James2013,
abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\~{}}without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
address = {New York},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1007/978-1-4614-7138-7},
edition = {1st Editio},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/kenno/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/James et al. - 2013 - An Introduction to Statistical Learning.pdf:pdf},
isbn = {978-1-4614-7137-0},
issn = {01621459},
pages = {441},
pmid = {10911016},
publisher = {Springer},
title = {{An Introduction to Statistical Learning}},
url = {http://www-bcf.usc.edu/{~}gareth/ISL/ISLR First Printing.pdf http://link.springer.com/10.1007/978-1-4614-7138-7},
volume = {103},
year = {2013}
}
@article{Turing1948,
author = {Turing, Alan},
journal = {National Physical Laboratory},
title = {{Intelligent machinery}},
year = {1948}
}
@article{Krizhevsky2009,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Krizhevsky, Alex},
doi = {10.1.1.222.9220},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/kenno/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky - 2009 - Learning Multiple Layers of Features from Tiny Images.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {{\ldots} Science Department, University of Toronto, Tech. {\ldots}},
pages = {1--60},
pmid = {25246403},
title = {{Learning Multiple Layers of Features from Tiny Images}},
url = {https://www.cs.toronto.edu/{~}kriz/learning-features-2009-TR.pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Learning+Multiple+Layers+of+Features+from+Tiny+Images{\#}0},
year = {2009}
}
@book{Jolliffe2002,
abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jolliffe, I T},
booktitle = {Encyclopedia of Statistics in Behavioral Science},
doi = {10.2307/1270093},
eprint = {arXiv:1011.1669v3},
file = {:E$\backslash$:/Nubes/Onedrive/OneDrive for Business/Proyectos de Investigaci{\'{o}}n/Proyecto-EARTH/Investigaci{\'{o}}n Bibliogr{\'{a}}fica/Jolliffe I. Principal Component Analysis (2ed., Springer, 2002)(518s){\_}MVsa{\_}.pdf:pdf},
isbn = {0387954422},
issn = {00401706},
number = {3},
pages = {487},
pmid = {21435900},
title = {{Principal Component Analysis}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/0470013192.bsa501/full},
volume = {30},
year = {2002}
}
@book{SistemaNacionaldeAcreditaciondelaEducacionSuperior2009,
address = {San Jos{\'{e}}, Costa Rica},
author = {{Sistema Nacional de Acreditaci{\'{o}}n de la Educaci{\'{o}}n Superior}},
file = {:C$\backslash$:/Users/kenno/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guillermo et al. - Unknown - Consejo Nacional de Acreditaci{\'{o}}n Direcci{\'{o}}n del SINAES.pdf:pdf},
pages = {79},
title = {{Manual de Acreditaci{\'{o}}n Oficial de Carreras de Grado del Sistema Nacional de Acreditaci{\'{o}}n de la Educaci{\'{o}}n Superior}},
url = {https://www.sinaes.ac.cr/documentos/Manual{\_}de{\_}Acreditacion{\_}Oficial{\_}de{\_}Carreras{\_}de{\_}Grado.pdf},
year = {2009}
}
@misc{DiPietro2016,
author = {DiPietro, Rob},
title = {{A Friendly Introduction to Cross-Entropy Loss}},
url = {https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/},
urldate = {2018-04-11},
year = {2016}
}
@misc{LinuxDocumentation,
author = {{Linux Documentation}},
title = {{Linux system calls Manpage}},
url = {http://manpages.ubuntu.com/manpages/trusty/man2/syscalls.2.html},
urldate = {2018-03-03}
}
